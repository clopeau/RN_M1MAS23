{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0mTO_jq6TVi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Un peu d'histoire\n",
        "\n",
        "Les réseaux de neurones artificiels, également appelés réseaux de neurones profonds, sont inspirés du fonctionnement des neurones biologiques du cerveau humain. Les premières recherches dans le domaine ont commencé dans les années 1940 et 1950, avec les travaux du mathématicien Warren McCulloch et du neurophysiologiste Walter Pitts, qui ont proposé un modèle de neurone artificiel basé sur les neurones biologiques.\n",
        "\n",
        "Frank Rosenblatt a proposé en 1958 le perceptron, un type spécifique de réseau de neurones à une couche qui prend des entrées pondérées et produit une sortie binaire en utilisant une fonction d'activation. Le perceptron est le premier réseau de neurones à avoir une architecture formelle et une règle d'apprentissage précise. L'algorithme d'apprentissage du perceptron, appelé règle de Rosenblatt, est un algorithme supervisé qui ajuste les poids des entrées pour minimiser l'erreur de classification. Le perceptron a été utilisé pour la reconnaissance de caractères manuscrits dans le projet \"The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain\" de Rosenblatt. \n",
        "\n",
        "Le premier théorème d'approximation universelle pour les réseaux de neurones, également appelé théorème d'approximation de Kolmogorov-Arnold, a été proposé indépendamment par Andrey Kolmogorov en 1957 et par Vladimir Arnold en 1964. Ce théorème stipule qu'un réseau de neurones à une seule couche cachée avec une fonction d'activation continue et non polynomiale peut approximer n'importe quelle fonction continue sur un sous-ensemble compact de l'espace euclidien de n'importe quelle dimension avec une précision arbitraire.\n",
        "\n",
        "Les perceptrons multicouches ont été développés pour améliorer les capacités de classification des perceptrons à une seule couche. Les premiers travaux sur les perceptrons multicouches ont été publiés en 1969 par Marvin Minsky et Seymour Papert dans leur livre intitulé \"Perceptrons: An Introduction to Computational Geometry\". Dans ce livre, Minsky et Papert ont montré que les perceptrons à une seule couche avaient des limitations en termes de capacité de représentation des fonctions non linéaires. Ils ont montré que les perceptrons à une seule couche ne pouvaient pas représenter des fonctions linéairement non séparables, ce qui limitait leur capacité à résoudre des problèmes de classification complexes.\n",
        "\n",
        "Paul Werbos a proposé en 1974 le concept de rétropropagation de l'erreur pour l'apprentissage de réseaux de neurones multicouches, ce qui a permis de modéliser des systèmes dynamiques complexes. En 1975, Werbos a utilisé des réseaux de neurones pour prédire la trajectoire de missiles dans son article \"Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences\".\n",
        "\n",
        "En 1976, Alex Waibel et Herbert Ney ont utilisé des réseaux de neurones à une couche cachée pour la reconnaissance de la parole dans le projet \"A Hybrid Neural Network for Phoneme Recognition\". Plus tard, en 1982, James Baker et al. ont utilisé des réseaux de neurones à plusieurs couches pour la reconnaissance de la parole dans le projet \"The DRAGON System - An Overview\".\n",
        "\n",
        "En 1986, David Rumelhart, Geoffrey Hinton et Ronald Williams, des chercheurs de l'université de Californie à San Diego, ont redécouvert et popularisé l'algorithme du rétropropagation du gradient dans un article intitulé \"Learning Representations by Back-Propagating Errors\". Cet article est considéré comme l'un des travaux fondateurs de l'apprentissage profond.\n",
        "\n",
        "Le travail de Rumelhart, Hinton et Williams a permis de relancer l'intérêt pour les réseaux de neurones profonds et a ouvert la voie à de nombreuses avancées dans le domaine de l'apprentissage profond. Aujourd'hui, la rétropropagation du gradient est largement utilisée comme méthode d'apprentissage pour les réseaux de neurones profonds et est considérée comme une étape cruciale dans la formation des réseaux de neurones modernes.\n",
        "\n",
        "En 1979, Kunihiko Fukushima a proposé le réseau de neurones convolutif (CNN) dans son article \"Neocognitron: A Self-organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position\". Les CNN ont été utilisés pour la reconnaissance de formes et la classification d'images dans les années 1980 et 1990, avec des projets tels que le \"Parallel Distributed Processing\" de David Rumelhart et James McClelland en 1986 et le \"Handwritten Digit Recognition using a Committee of Neural Networks\" de Yann LeCun et al. en 1990.\n",
        "\n",
        "En résumé, l'histoire des réseaux de neurones artificiels commence dans les années 1940 et 1950 avec les travaux de Warren McCulloch et Walter Pitts. Au fil des décennies, des progrès significatifs ont été réalisés dans le domaine, en particulier dans les années 1980 et 1990 avec l'émergence des réseaux de neurones profonds et des réseaux de convolution pour la reconnaissance de motifs et d'images. Aujourd'hui, les réseaux de neurones profonds sont largement utilisés dans de nombreuses applications, notamment la reconnaissance de la parole, la reconnaissance d'images, la traduction automatique et le traitement du langage naturel."
      ],
      "metadata": {
        "id": "LKhh_DM86jDI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Principe d'un réseau de neurone\n",
        "\n"
      ],
      "metadata": {
        "id": "_LRzNXuNIDYy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Le perceptron\n",
        "\n",
        "Un réseau de neurones sans couche cachée est également appelé \"perceptron\". Il s'agit d'un modèle d'apprentissage automatique utilisé pour la classification binaire, c'est-à-dire pour séparer deux classes de données.\n",
        "\n",
        "Le principe mathématique de ce modèle consiste à prendre une entrée (ou un ensemble d'entrées), à pondérer chaque entrée par un poids et à sommer les résultats pondérés pour produire une sortie. La sortie est ensuite comparée à un seuil de décision pour déterminer la classe de la donnée d'entrée.\n",
        "\n",
        "Plus formellement, si on note :\n",
        "\n",
        "* $x$ l'entrée du réseau,\n",
        "* $w$ les poids du réseau,\n",
        "* $b$ le biais du réseau,\n",
        "* $f$ la fonction d'activation,\n",
        "on peut écrire mathématiquement :\n",
        "\n",
        "$$y=f(wx+b)$$\n",
        "\n",
        "où $f$ est généralement une fonction d'activation simple comme la fonction seuil ou la fonction sigmoïde, qui convertit la sortie pondérée en une valeur comprise entre 0 et 1.\n",
        "\n",
        "Le perceptron est un modèle linéaire, il sépare les données suivant un hyperplan. ce modèle ne peut pas résoudre des problèmes de classification non linéaires. Pour des problèmes plus complexes, des réseaux de neurones avec des couches cachées sont utilisés pour capturer des relations non linéaires entre les entrées et les sorties.\n",
        "\n"
      ],
      "metadata": {
        "id": "JeaSdndhEaoB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Réseau multicouche\n",
        "Pour une entrée $\\mathbf{x}$ de taille $n_0$, la propagation avant dans un réseau de neurones à $n$ couches peut être décrite par les équations suivantes :\n",
        "\n",
        "\\begin{equation*}\n",
        "\\mathbf{z}^l = \\mathbf{W}^l\\mathbf{y}^{l-1} + \\mathbf{b}^l\n",
        "\\end{equation*}\n",
        "\n",
        "\\begin{equation*}\n",
        "\\mathbf{y}^l = f(\\mathbf{z}^l)\n",
        "\\end{equation*}\n",
        "\n",
        "où \n",
        "* $\\mathbf{y}^l$ est le vecteur des sorties des neurones dans la couche $l$, \n",
        "* $\\mathbf{W}^l$ est la matrice des poids des connexions entre les neurones des couches $l-1$ et $l$, \n",
        "* $\\mathbf{b}^l$ est le vecteur des biais des neurones de la couche $l$ et $f$ est une fonction d'activation non linéaire appliquée à chaque élément du vecteur des sorties pondérées de la couche $l$.\n",
        "\n",
        "Le réseau de neurones a donc $n$ couches, avec $n-1$ couches cachées et une couche de sortie. La taille de la couche d'entrée est $n_0$, la taille de la couche de sortie est $n_n$, et la taille de la couche cachée $l$ est $n_l$. Les dimensions de $\\mathbf{W}^l$ sont donc $n_l \\times n_{l-1}$ et celles de $\\mathbf{b}^l$ sont $n_l \\times 1$ pour chaque couche $l$.\n",
        "​\n",
        "La formule finale du réseau neuronal obtenu par composition des fonctions est :\n",
        "\n",
        "\\begin{equation*}\n",
        "\\mathbf{y}(\\mathbf{x}) = f^{(n)}(\\mathbf{W}^n f^{(n-1)}(\\mathbf{W}^{n-1} \\cdots f^{(2)}(\\mathbf{W}^2 f^{(1)}(\\mathbf{W}^1 \\mathbf{x} + \\mathbf{b}^1) + \\mathbf{b}^2) \\cdots + \\mathbf{b}^{n-1}) + \\mathbf{b}^n)\n",
        "\\end{equation*}\n",
        "\n",
        "où $\\mathbf{x}$ est un vecteur d'entrée, $\\mathbf{y}(\\mathbf{x})$ est le vecteur de sortie du réseau, $f^{(l)}$ est la fonction d'activation non linéaire appliquée aux sorties pondérées de la couche $l$.\n",
        "\n",
        "Ce réseau de neurones a $n$ couches, avec une couche d'entrée de dimension appropriée pour le problème en question, $n-1$ couches cachées, et une couche de sortie de dimension appropriée pour le problème en question. Chaque couche est connectée à la couche précédente par une matrice de poids et un vecteur de biais.\n"
      ],
      "metadata": {
        "id": "QG7Cw6b3IJke"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apprentissage supervisé\n",
        "\n",
        "L'apprentissage supervisé (ou subordonné) consiste à fournir au réseau des exemples d'entraînement afin qu'il apprenne à effectuer une tâche spécifique.\n",
        "\n",
        "Un jeu de données est généralement constitué de deux parties :\n",
        "\n",
        "1. Les données d'entrée : il s'agit des exemples de données qui seront fournis au réseau de neurones en entrée. Ces données peuvent prendre différentes formes selon le type de tâche à effectuer, mais elles sont généralement représentées sous forme de vecteurs ou de tenseurs. Par exemple, pour une tâche de classification d'images, les données d'entrée pourraient être des images sous forme de matrices de pixels.\n",
        "\n",
        "2. Les étiquettes de sortie : il s'agit des valeurs attendues en sortie pour chaque exemple de données d'entrée. Pour une tâche de classification, les étiquettes de sortie seraient des catégories ou des classes auxquelles appartiennent les données d'entrée.\n",
        "\n",
        "Autrement dit cela revient à avoir un ensemble de données \n",
        "\n",
        "$$\\mathbf{S}_{apprentissage}=\\Big\\{ (\\mathbf{x}_i,\\mathbf{y}_i) \\quad i \\in I \\Big\\}$$\n",
        "\n",
        "\n",
        "Le but de l'apprentissage est de trouver une fonction qui transforme les données d'entrée en étiquettes de sortie de manière précise et fiable. Pour cela, le réseau de neurones est entraîné sur un jeu de données d'entraînement en ajustant les poids des neurones à chaque itération jusqu'à ce que la fonction de perte soit minimisée. Une fois l'entraînement terminé, le réseau de neurones peut être utilisé pour effectuer des prédictions sur de nouvelles données.\n",
        "\n",
        "Il est important que le jeu de données d'entraînement soit représentatif de la distribution des données que le modèle sera amené à rencontrer en pratique, afin que le réseau de neurones puisse généraliser efficacement à de nouvelles données. De plus, pour éviter que le modèle ne surapprenne aux données d'entraînement, il est courant de diviser le jeu de données en un ensemble d'entraînement et un ensemble de validation ou de test, sur lesquels la performance du modèle est évaluée de manière indépendante."
      ],
      "metadata": {
        "id": "qkRAkoihN2cg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fonction d'objectif (Loss)\n",
        "\n",
        "L'utilisation de la fonction de perte (ou \"loss\" en anglais) en réseau de neurones est motivée par le fait que le but de l'apprentissage supervisé est d'ajuster les poids du réseau de neurones de manière à minimiser la différence entre les prédictions du modèle et les valeurs réelles des données d'entraînement. La fonction de perte est donc utilisée pour mesurer cette différence et fournir une indication de la qualité des prédictions du modèle.\n",
        "\n",
        "Le terme \"loss\" en anglais peut être traduit en français par \"perte\" ou \"pertes\". Le nom \"loss\" est utilisé plutôt que \"coût\" ou \"objectif\" car il décrit plus précisément la fonction de perte en tant que mesure de la différence entre les prédictions du modèle et les valeurs réelles. En d'autres termes, la fonction de perte mesure la \"perte\" ou la \"perturbation\" subie par le modèle lorsqu'il prédit des valeurs différentes de celles attendues.\n",
        "\n",
        "Le terme \"coût\" ou \"objectif\" est également parfois utilisé pour désigner la fonction de perte en apprentissage supervisé, mais le terme \"loss\" reste le plus couramment utilisé dans la communauté de l'apprentissage automatique.\n",
        "\n",
        "La fonction Loss exprime une distance ou erreur entre la prédiction du réseau $\\mathbf{y}(\\mathbf{x}_i)$ et la donnée $\\mathbf{y}_i$ pour $i \\in I$ (ensemble d'apprentissage). Généralement elle prend en compte l'ensemble des données d'apprentissage sous forme de sommation\n",
        "\n",
        "$$\\mathbf{L} = \\frac{1}{N} \\sum_{i\\in I} L\\big(\\mathbf{y}_i,\\mathbf{y}(\\mathbf{x}_i)\\big)$$\n",
        "avec **N** est le nombre d'échantillons.\n",
        "\n",
        "Il existe plusieurs types de fonctions de perte, qui sont choisies en fonction du type de tâche que le modèle doit résoudre. \n",
        "\n",
        "Voici quelques exemples usuels :\n",
        "\n",
        "* Pour les tâches de classification binaire (par exemple, prédire si une image contient un chat ou un chien), on utilise souvent la fonction de perte de l'entropie croisée binaire (\"binary cross-entropy\" en anglais).\n",
        "$$-\\frac{1}{N}\\sum_{i=1}^{N}[y_i\\log(p_i)+(1-y_i)\\log(1-p_i)]$$\n",
        "où : **N** est le nombre d'échantillons dans le jeu de données, $y_i$ est la vraie valeur binaire (0 ou 1) de l'échantillon i, $p_i$ est la probabilité prédite par le modèle pour l'échantillon i.\n",
        "La Binary Cross-Entropy mesure la différence entre les probabilités prédites et les vraies valeurs binaires, et elle est utilisée comme fonction de perte pour les problèmes de classification binaire. Cette fonction de perte est plus sensible aux erreurs de classification que la Mean Squared Error, ce qui la rend plus adaptée pour les problèmes de classification binaire.\n",
        "\n",
        "* Pour les tâches de classification multiclasse (par exemple, prédire la catégorie d'un article de presse), on utilise souvent la fonction de perte de l'entropie croisée catégorielle (\"categorical cross-entropy\" en anglais).\n",
        "$$-\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{j=1}^{C}y_{i,j}\\log(p_{i,j})$$\n",
        "où : **N** est le nombre d'échantillons dans le jeu de données, **C** est le nombre de classes,\n",
        "$y_{i,j}$ est un indicateur binaire qui indique si l'échantillon i appartient à la classe j (1 si oui, 0 sinon), \n",
        "$p_{i,j}$ est la probabilité prédite par le modèle pour l'échantillon i appartenant à la classe j.\n",
        "La Categorical Cross-Entropy mesure la différence entre les probabilités prédites et les vraies valeurs de classification multi-classes. Cette fonction de perte est utilisée pour entraîner des modèles de classification multi-classes avec des étiquettes de classification codées en \"one-hot\". Elle est plus sensible aux erreurs de classification que la Mean Squared Error, ce qui la rend plus adaptée pour les problèmes de classification multi-classes.\n",
        "\n",
        "* Pour les tâches de régression (par exemple, prédire le prix d'une maison en fonction de ses caractéristiques), on utilise souvent la fonction de perte de l'erreur quadratique moyenne (\"mean squared error\" en anglais).\n",
        "\n",
        "En général, la fonction de perte doit être choisie avec soin en fonction de la tâche à accomplir, afin d'assurer que le modèle est capable de produire des prédictions précises et cohérentes."
      ],
      "metadata": {
        "id": "W2mjka7sIATY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rétropropagation du gradient\n",
        "\n",
        "On doit minimiser la fonction Loss pour cela si on revient à l'écriture de celle ci. Une composante est de la forme\n",
        "$$L\\big(\\mathbf{y}_i,\\mathbf{y}(\\mathbf{x}_i)\\big)=L\\big(\\mathbf{y}_i,\\mathbf{f^n \\circ f^{n-1}\\circ \\dots \\circ f^0}(\\mathbf{x}_i)\\big)$$\n",
        "\n",
        "a compléter"
      ],
      "metadata": {
        "id": "B9Zyx0QiXU3M"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IIh7oBAoNjY8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}