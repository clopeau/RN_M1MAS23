{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPxv+br5diJ9iBTgIwomKv0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Fonctions d'activation disponibles dans Keras de TensorFlow :\n","\n","\n","* **Sigmoïde (Sigmoid)**: La fonction sigmoïde est une fonction d'activation qui transforme les valeurs d'entrée en une plage de 0 à 1. Cette fonction est couramment utilisée pour les problèmes de classification binaire.\n","\n","* **Tangente hyperbolique (Tanh)** : La fonction tanh est similaire à la sigmoïde, mais elle transforme les valeurs d'entrée en une plage de -1 à 1. Cette fonction est également couramment utilisée pour les problèmes de classification.\n","\n","* **ReLU (Rectified Linear Unit)** : La fonction ReLU est une fonction d'activation qui transforme les valeurs d'entrée en une plage de 0 à l'infini. Cette fonction est couramment utilisée pour les problèmes de classification et de régression.\n","\n","* **Softmax** : La fonction softmax est une fonction d'activation qui transforme les valeurs d'entrée en une distribution de probabilité, où la somme des valeurs de sortie est égale à 1. Cette fonction est couramment utilisée pour les problèmes de classification avec plusieurs classes.\n","\n","* **Leaky ReLU** : La fonction Leaky ReLU est une variante de la fonction ReLU qui permet aux valeurs négatives d'être légèrement activées. Cette fonction est couramment utilisée pour les problèmes de classification et de régression.\n","\n","* **ELU (Exponential Linear Unit)** : La fonction ELU est une fonction d'activation qui est similaire à la fonction ReLU, mais elle permet aux valeurs négatives d'être activées de manière exponentielle. Cette fonction est couramment utilisée pour les problèmes de classification et de régression.\n","\n","* **SELU (Scaled Exponential Linear Unit)** : La fonction SELU est une variante de la fonction ELU qui est conçue pour être auto-normalisante. Cette fonction est couramment utilisée pour les réseaux de neurones à propagation avant profonde.\n","\n","\n","\n","\n","\n"],"metadata":{"id":"ZeMUJ8bCEQyg"}},{"cell_type":"markdown","source":["# Fonctions de Scikit-learn pour encoder des labels :\n","\n","* **LabelEncoder** : La classe LabelEncoder permet de convertir des étiquettes de texte en valeurs numériques entières. Cette classe est couramment utilisée pour encoder les étiquettes de classes pour les problèmes de classification.\n","\n","* **OneHotEncoder** : La classe OneHotEncoder permet de convertir des étiquettes en vecteurs binaires à codage unique. Chaque vecteur binaire représente une seule classe, où la valeur 1 indique que la classe est présente et la valeur 0 indique que la classe est absente. Cette classe est couramment utilisée pour les problèmes de classification avec plusieurs classes.\n","\n","* **LabelBinarizer** : La classe LabelBinarizer permet de convertir des étiquettes de texte en vecteurs binaires. Cette classe est similaire à la classe OneHotEncoder, mais elle ne crée qu'un seul vecteur binaire pour chaque échantillon, ce qui est utile pour les problèmes de classification binaire.\n","\n","* **MultiLabelBinarizer** : La classe MultiLabelBinarizer permet de convertir des étiquettes de texte en vecteurs binaires pour les problèmes de classification avec plusieurs étiquettes. Cette classe crée un vecteur binaire pour chaque étiquette, indiquant si l'étiquette est présente ou absente pour chaque échantillon.\n","\n","* **OrdinalEncoder** : La classe OrdinalEncoder permet de convertir des étiquettes de texte en valeurs numériques ordinales. Cette classe est similaire à la classe LabelEncoder, mais elle prend en compte l'ordre des étiquettes."],"metadata":{"id":"Sz7yR1rqFfLh"}},{"cell_type":"code","source":["from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n","\n","# Créer un exemple d'ensemble de données d'étiquettes de classe\n","labels = ['chat', 'chien', 'oiseau', 'chien', 'chat', 'oiseau']\n","\n","# Créer une instance de LabelEncoder\n","le = LabelEncoder()\n","\n","# Adapter et transformer les étiquettes en valeurs numériques\n","numeric_labels = le.fit_transform(labels)\n","\n","# Imprimer les étiquettes numériques\n","print(numeric_labels)"],"metadata":{"id":"EbdLjkuwOI0s","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679044565087,"user_tz":-60,"elapsed":2,"user":{"displayName":"Thierry .Clopeau","userId":"16944828332985793882"}},"outputId":"53806044-d708-4f74-a11b-1d6b193a1ad5"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["[0 1 2 1 0 2]\n"]}]},{"cell_type":"markdown","source":["# Fonctions de score disponibles dans Keras :\n","\n","* **Accuracy** : La fonction d'accuracy calcule la proportion d'échantillons correctement classés par le modèle. Cette fonction est couramment utilisée pour les problèmes de classification.\n","$$Accuracy = \\frac{TP+TN}{TP+TN+FP+FN}$$\n","où **TP** est le nombre de vrais positifs, **TN** est le nombre de vrais négatifs, **FP** est le nombre de faux positifs et **FN** est le nombre de faux négatifs.\n","\n","* **Precision** : La fonction de Precision calcule la proportion des prédictions positives qui sont correctes parmi toutes les prédictions positives. Cette fonction est couramment utilisée pour les problèmes de classification.\n","$$Precision = \\frac{TP}{TP+FP}$$\n","\n","* **Recall** : La fonction Recall calcule la proportion des vrais positifs qui sont correctement identifiés parmi tous les vrais positifs et les faux négatifs. Cette fonction est couramment utilisée pour les problèmes de classification.\n","$$Recall = \\frac{TP}{TP+FN}$$\n","\n","* **F1 Score** : La fonction F1 Score est une mesure de la précision et du recall, qui calcule la moyenne harmonique entre les deux mesures. Cette fonction est couramment utilisée pour les problèmes de classification.\n","$$F1 = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall}$$\n","\n","* **Binary Accuracy** : La fonction Binary Accuracy calcule l'accuracy pour les problèmes de classification binaire.\n","\n","* **Categorical Accuracy** : La fonction Categorical Accuracy calcule l'accuracy pour les problèmes de classification multi-classes.\n","\n","* **Sparse Categorical Accuracy** : La fonction Sparse Categorical Accuracy calcule l'accuracy pour les problèmes de classification multi-classes avec des étiquettes d'entiers.\n","\n","* **Top k Categorical Accuracy**: La fonction Top k Categorical Accuracy calcule l'accuracy pour les problèmes de classification multi-classes où l'on s'intéresse aux k classes les plus probables.\n","\n","* **Mean Squared Error (MSE)** : La fonction de MSE calcule la moyenne des carrés des différences entre les prédictions et les vraies valeurs. Cette fonction est couramment utilisée pour les problèmes de régression.\n","\n","* **Root Mean Squared Error (RMSE)** : La fonction de RMSE calcule la racine carrée de la moyenne des carrés des différences entre les prédictions et les vraies valeurs. Cette fonction est également couramment utilisée pour les problèmes de régression."],"metadata":{"id":"9hpdv4qKGhES"}},{"cell_type":"markdown","source":["# Fonctions Loss disponibles dans Keras :\n","\n","* **Mean Squared Error (MSE)** : La fonction de MSE calcule la moyenne des carrés des différences entre les prédictions et les vraies valeurs. Cette fonction est couramment utilisée pour les problèmes de régression.\n","\n","* **Mean Absolute Error (MAE)** : La fonction de MAE calcule la moyenne des valeurs absolues des différences entre les prédictions et les vraies valeurs. Cette fonction est également couramment utilisée pour les problèmes de régression.\n","\n","* **Binary Cross-Entropy** : La fonction de Binary Cross-Entropy calcule l'entropie croisée entre les prédictions et les vraies valeurs pour les problèmes de classification binaire.\n","$$-\\frac{1}{N}\\sum_{i=1}^{N}[y_i\\log(p_i)+(1-y_i)\\log(1-p_i)]$$\n","où : **N** est le nombre d'échantillons dans le jeu de données, $y_i$ est la vraie valeur binaire (0 ou 1) de l'échantillon i, $p_i$ est la probabilité prédite par le modèle pour l'échantillon i.\n","La Binary Cross-Entropy mesure la différence entre les probabilités prédites et les vraies valeurs binaires, et elle est utilisée comme fonction de perte pour les problèmes de classification binaire. Cette fonction de perte est plus sensible aux erreurs de classification que la Mean Squared Error, ce qui la rend plus adaptée pour les problèmes de classification binaire.\n","\n","* **Categorical Cross-Entropy** : La fonction de Categorical Cross-Entropy calcule l'entropie croisée entre les prédictions et les vraies valeurs pour les problèmes de classification multi-classes.\n","$$-\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{j=1}^{C}y_{i,j}\\log(p_{i,j})$$\n","où : **N** est le nombre d'échantillons dans le jeu de données, **C** est le nombre de classes,\n","$y_{i,j}$ est un indicateur binaire qui indique si l'échantillon i appartient à la classe j (1 si oui, 0 sinon), \n","$p_{i,j}$ est la probabilité prédite par le modèle pour l'échantillon i appartenant à la classe j.\n","La Categorical Cross-Entropy mesure la différence entre les probabilités prédites et les vraies valeurs de classification multi-classes. Cette fonction de perte est utilisée pour entraîner des modèles de classification multi-classes avec des étiquettes de classification codées en \"one-hot\". Elle est plus sensible aux erreurs de classification que la Mean Squared Error, ce qui la rend plus adaptée pour les problèmes de classification multi-classes.\n","\n","* **Sparse Categorical Cross-Entropy** : La fonction de Sparse Categorical Cross-Entropy calcule l'entropie croisée entre les prédictions et les vraies valeurs pour les problèmes de classification multi-classes avec des étiquettes d'entiers.\n","$$-\\frac{1}{N}\\sum_{i=1}^{N}\\log(p_{i,y_i})$$\n","où : **N** est le nombre d'échantillons dans le jeu de données,\n","$y_i$ est l'étiquette d'entier de l'échantillon i (un entier qui représente la classe à laquelle appartient l'échantillon),\n","$p_{i,j}$ est la probabilité prédite par le modèle pour l'échantillon i appartenant à la classe j.\n","La Sparse Categorical Cross-Entropy mesure la différence entre les probabilités prédites et les vraies valeurs de classification multi-classes, lorsque les étiquettes de classification sont des entiers plutôt que des vecteurs codés en \"one-hot\". Cette fonction de perte est utilisée pour entraîner des modèles de classification multi-classes lorsque les étiquettes de classification sont des entiers. Elle est plus sensible aux erreurs de classification que la Mean Squared Error, ce qui la rend plus adaptée pour les problèmes de classification multi-classes.\n","\n","* **Hinge Loss** : La fonction de Hinge Loss calcule la perte de charnière pour les problèmes de classification binaire ou multi-classes.\n","$$\\max(0, 1 - y_{true}f(x))$$\n","où :\n","$y_{true}$ est la vraie étiquette de classification (-1 ou 1),\n","f(x) est la sortie prédite pour l'échantillon x.\n","La Hinge Loss est une fonction de perte utilisée pour l'apprentissage des machines à vecteurs de support (SVM) pour la classification binaire. Elle est utilisée pour maximiser la marge entre les classes en minimisant les erreurs de classification. Elle est souvent utilisée pour les problèmes de classification binaire avec des modèles linéaires. Si la sortie prédite pour l'échantillon x a le signe correct (c'est-à-dire que la prédiction est dans la même direction que la vraie étiquette de classification), alors la Hinge Loss est égale à zéro. Sinon, elle est proportionnelle à la distance entre la sortie prédite et la vraie étiquette de classification.\n","\n","* **Kullback-Leibler Divergence** : La fonction de Kullback-Leibler Divergence calcule la divergence de Kullback-Leibler entre les prédictions et les vraies valeurs.\n","$$D_{KL}(P||Q) = \\sum_{i} P(i) \\log\\frac{P(i)}{Q(i)}$$\n","où :\n","P et Q sont deux distributions de probabilité sur le même espace d'événements,\n","i représente un événement dans l'espace d'événements,\n","$P(i)$ est la probabilité de l'événement i sous la distribution P,\n","$Q(i)$ est la probabilité de l'événement i sous la distribution Q.\n","La KL divergence mesure la distance entre deux distributions de probabilité. Elle est asymétrique, c'est-à-dire que la KL divergence de P à Q n'est pas la même que celle de Q à P. La KL divergence mesure la quantité d'information que l'on perd en utilisant une distribution Q pour approximer une distribution P. Elle est souvent utilisée comme fonction de perte pour des modèles de classification et de régression, lorsque la sortie du modèle est une distribution de probabilité et que l'on souhaite minimiser la distance entre la distribution prédite et la distribution réelle.\n","\n","* **Poisson** : La fonction de Poisson calcule la perte de Poisson pour les problèmes de régression de Poisson.\n","$$L(y, \\hat{y}) = \\hat{y} - y\\log(\\hat{y})$$\n","où :\n","$y$ est la vraie valeur,\n","$\\hat{y}$ est la valeur prédite.\n","La fonction de perte Poisson est souvent utilisée dans les modèles de régression pour prédire des valeurs positives, telles que le nombre de fois où un événement se produit. Elle est particulièrement utile lorsque les données suivent une distribution de Poisson, qui est une distribution de probabilité discrète souvent utilisée pour modéliser des événements rares. La fonction de perte Poisson mesure la distance entre les valeurs prédites et les vraies valeurs à l'aide de la log-vraisemblance de la distribution de Poisson. Elle est conçue pour minimiser la divergence entre la distribution de Poisson prédite et la distribution de Poisson réelle des données.\n","* **Cosine Similarity** : La fonction de Cosine Similarity calcule la distance cosinus entre les prédictions et les vraies valeurs.\n","$$\\text{similarity}(x,y) = \\frac{x\\cdot y}{|x|\\cdot|y|}$$\n","où :\n","$x$ et $y$ sont deux vecteurs,\n","$x\\cdot y$ est leur produit scalaire,\n","$|x|$ et $|y|$ sont les normes euclidiennes des vecteurs $x$ et $y$.\n","La similarité cosinus mesure l'angle entre deux vecteurs dans l'espace vectoriel. Elle est utilisée pour mesurer la similarité entre deux vecteurs de caractéristiques, tels que les vecteurs de représentation de mots dans le traitement du langage naturel. Plus la valeur de la similarité cosinus est proche de 1, plus les vecteurs sont similaires, tandis que plus la valeur est proche de 0, plus les vecteurs sont différents. La similarité cosinus est souvent utilisée comme mesure de similarité pour des tâches telles que la recherche de documents et la recommandation de produits."],"metadata":{"id":"75vrkRtVHkYp"}},{"cell_type":"code","source":[],"metadata":{"id":"GgVtkMKtIHje"},"execution_count":null,"outputs":[]}]}